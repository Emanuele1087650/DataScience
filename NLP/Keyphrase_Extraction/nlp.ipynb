{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install keybert\n",
        "!pip install textacy\n",
        "!python -m spacy download en"
      ],
      "metadata": {
        "id": "6Ry8tmj37V7H",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import textacy\n",
        "\n",
        "# Spacy model\n",
        "nlp = textacy.load_spacy_lang(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "RGUqW4waTPVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = \" \".join(token.lemma_ for token in doc)\n",
        "    return tokens\n",
        "\n",
        "def preprocess_keyphrase(text):\n",
        "    tokens = []\n",
        "    for t in text:\n",
        "        doc = nlp(t.lower())\n",
        "        doc = \" \".join(token.lemma_ for token in doc)\n",
        "        tokens.append(doc)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "NQrvzX6I9WUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments, Seq2SeqTrainer\n",
        "import os\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import torch\n",
        "from sklearn.metrics import jaccard_score\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definisci il percorso di salvataggio su Drive\n",
        "drive_base_path = \"/content/drive/MyDrive/T5_Keyphrase_Extraction\"\n",
        "os.makedirs(drive_base_path, exist_ok=True)\n",
        "model_save_path = os.path.join(drive_base_path, \"trained_model\")\n",
        "\n",
        "# Percorsi per dataset tokenizzato e modello\n",
        "tokenized_dataset_path = os.path.join(drive_base_path, \"tokenized_dataset\")\n",
        "\n",
        "# Carica il dataset Inspec\n",
        "# Assumiamo che ogni esempio contenga le chiavi \"abstract\" e \"keyphrases\" (quest'ultima è una lista di stringhe)\n",
        "dataset = load_dataset(\"taln-ls2n/inspec\", split=\"train\", trust_remote_code=True)\n",
        "val_dataset = load_dataset(\"taln-ls2n/inspec\", split=\"validation\", trust_remote_code=True)\n",
        "test_dataset = load_dataset(\"taln-ls2n/inspec\", split=\"test\", trust_remote_code=True)\n",
        "\n",
        "dataset = dataset.map(lambda x: {\"abstract\": lemmatize(x[\"abstract\"])})\n",
        "dataset = dataset.map(lambda x: {\"title\": lemmatize(x[\"title\"])})\n",
        "dataset = dataset.map(lambda x: {\"keyphrases\": preprocess_keyphrase(x[\"keyphrases\"])})\n",
        "\n",
        "# Preprocessamento: creiamo l'input concatenando il prompt con l'abstract\n",
        "# e il target come stringa con le keyphrase separate da virgola.\n",
        "def preprocess(example):\n",
        "    input_text = \"Extract keyphrases: \" + example[\"title\"]+\". \"+example[\"abstract\"]\n",
        "    # Se keyphrases è una lista, uniscile in una singola stringa separate da virgole\n",
        "    if isinstance(example[\"keyphrases\"], list):\n",
        "        target_text = \", \".join(example[\"keyphrases\"])\n",
        "    else:\n",
        "        target_text = example[\"keyphrases\"]\n",
        "    return {\"input_text\": input_text, \"target_text\": target_text}\n",
        "\n",
        "# Applichiamo il preprocessamento\n",
        "dataset = dataset.map(preprocess)\n",
        "val_dataset = val_dataset.map(preprocess)\n",
        "\n",
        "test_dataset = test_dataset.map(lambda x: {\"abstract\": lemmatize(x[\"abstract\"])})\n",
        "test_dataset = test_dataset.map(lambda x: {\"title\": lemmatize(x[\"title\"])})\n",
        "test_dataset = test_dataset.map(lambda x: {\"keyphrases\": preprocess_keyphrase(x[\"keyphrases\"])})\n",
        "test_dataset = test_dataset.map(preprocess)"
      ],
      "metadata": {
        "id": "XJce0rx40--1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scegliamo il modello T5 (in questo esempio T5-small, ma puoi scegliere una versione più grande se necessario)\n",
        "model_checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Funzione per tokenizzare gli esempi\n",
        "def tokenize_function(example):\n",
        "    # Tokenizza l'input\n",
        "    model_inputs = tokenizer(example[\"input_text\"], max_length=512, truncation=True)\n",
        "    # Tokenizza il target con il tokenizer dedicato per le sequenze di destinazione\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example[\"target_text\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Applichiamo la tokenizzazione al dataset (processo in batch per velocità)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "val_tokenized_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Impostiamo il data collator per il sequence-to-sequence task\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "aQlHt9GB4Wy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salva il tokenized_datasets in una directory specifica\n",
        "dataset.save_to_disk(os.path.join(drive_base_path, \"train_tokenized_dataset\"))\n",
        "val_dataset.save_to_disk(os.path.join(drive_base_path, \"val_tokenized_dataset\"))\n",
        "test_dataset.save_to_disk(os.path.join(drive_base_path, \"test_tokenized_dataset\"))"
      ],
      "metadata": {
        "id": "GvZmhSjyYKqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "dataset = load_from_disk(os.path.join(drive_base_path, \"train_tokenized_dataset\"))\n",
        "val_dataset = load_from_disk(os.path.join(drive_base_path, \"val_tokenized_dataset\"))\n",
        "test_dataset = load_from_disk(os.path.join(drive_base_path, \"test_tokenized_dataset\"))"
      ],
      "metadata": {
        "id": "jKsTdnaUYKFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definiamo i parametri dell'addestramento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(model_save_path, \"t5-inspec\"),\n",
        "    logging_strategy=\"epoch\",            # Logga alla fine di ogni epoca\n",
        "    per_device_train_batch_size=8,     # Batch size per l'addestramento\n",
        "    num_train_epochs=8,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Utilizziamo il Seq2SeqTrainer per compiti di generazione\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Avvia l'addestramento: ad ogni epoca il trainer stamperà i log, includendo loss e le metriche (es. accuratezza)\n",
        "trainer.train()\n",
        "\n",
        "# Salva il modello fine-tuned\n",
        "model_save_path = os.path.join(training_args.output_dir, \"final_model\")\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "JkJ6a3hz4aor",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salva il modello addestrato\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "PEuqVuJdXLpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il modello salvato\n",
        "model = T5ForConditionalGeneration.from_pretrained(os.path.join(model_save_path, \"/t5-inspec/runs/Apr11_15-20-01_1d01f3dc3706\"))\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "mgMzWD40XMVV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione per calcolare la similarità di Jaccard\n",
        "def jaccard_similarity(list1, list2):\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    return intersection / union if union != 0 else 0.0"
      ],
      "metadata": {
        "id": "anKbev5cLLEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, tokenizer, test_dataset):\n",
        "    model.eval().to(\"cpu\")  # Imposta il modello in modalità di valutazione su CPU\n",
        "    jaccard_scores = []  # Lista per memorizzare i punteggi di similarità\n",
        "\n",
        "    for i, example in enumerate(test_dataset):\n",
        "        # Tokenizza l'input (abstract)\n",
        "        inputs = tokenizer(example[\"input_text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        # Predici le keyphrase\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, num_beams=4, max_length=128, early_stopping=True)\n",
        "\n",
        "        # Decodifica l'output\n",
        "        predicted_keyphrases = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predicted_keyphrases_list = predicted_keyphrases.split(',')  # Lista di keyphrase predette\n",
        "        predicted_keyphrases_list = [phrase.strip() for phrase in predicted_keyphrases_list]  # Rimuovi spazi bianchi\n",
        "        real_keyphrases_list = example[\"keyphrases\"]\n",
        "\n",
        "        # Calcola la similarità di Jaccard tra le keyphrase reali e quelle predette\n",
        "        jaccard_score_value = jaccard_similarity(real_keyphrases_list, predicted_keyphrases_list)\n",
        "        jaccard_scores.append(jaccard_score_value)\n",
        "\n",
        "        print(\"Input (Abstract):\", example[\"input_text\"])\n",
        "        print(\"Real Keyphrases:\", example[\"keyphrases\"])\n",
        "        print(\"Predicted Keyphrases:\", predicted_keyphrases_list)\n",
        "        print(\"Jaccard Similarity:\", jaccard_score_value)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Calcola la media della similarità di Jaccard\n",
        "    average_jaccard_score = np.mean(jaccard_scores)\n",
        "    print(\"Average Jaccard Similarity:\", average_jaccard_score)\n",
        "\n",
        "# Esegui il testing sul dataset di test\n",
        "test_model(model, tokenizer, tokenized_test_dataset)\n"
      ],
      "metadata": {
        "id": "fmrfGxJDrTXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEXTACY\n",
        "\n",
        "textacy_keyphrases = []\n",
        "real_keyphrases_list = []\n",
        "jaccard_scores = []\n",
        "for elem in test_dataset:\n",
        "  doc = textacy.make_spacy_doc(elem['input_text'], lang=nlp)\n",
        "  textacy_keyphrases_current = [kps for kps, weights in textacy.extract.keyterms.textrank(doc, normalize=\"lemma\", topn=8)]\n",
        "  textacy_keyphrases.append(textacy_keyphrases_current)\n",
        "  real_keyphrases_list = elem[\"keyphrases\"]\n",
        "  jaccard_score_value = jaccard_similarity(real_keyphrases_list, textacy_keyphrases_current)\n",
        "  jaccard_scores.append(jaccard_score_value)\n",
        "  print(\"Real Keyphrases:\", elem[\"keyphrases\"])\n",
        "  print(\"Predicted Keyphrases:\", textacy_keyphrases[-1])\n",
        "  print(\"Jaccard Similarity:\", jaccard_score_value)\n",
        "\n",
        "# Calcola la media della similarità di Jaccard\n",
        "average_jaccard_score = np.mean(jaccard_scores)\n",
        "print(\"Average Jaccard Similarity:\", average_jaccard_score)"
      ],
      "metadata": {
        "id": "-dcJnnd0HfO-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KEYBERT\n",
        "\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# Istanzia il modello (puoi usare anche modelli diversi)\n",
        "kw_model = KeyBERT(model='all-MiniLM-L6-v2')  # modello leggero, ma efficace\n",
        "\n",
        "keybert_keyphrases = []\n",
        "real_keyphrases_list = []\n",
        "jaccard_scores = []\n",
        "for elem in test_dataset:\n",
        "  keybert_keyphrases_current = [kw for kw, score in kw_model.extract_keywords(elem['input_text'], keyphrase_ngram_range=(1, 5), stop_words='english', top_n=8)]\n",
        "  keybert_keyphrases.append(keybert_keyphrases_current)\n",
        "  real_keyphrases_list = elem[\"keyphrases\"]\n",
        "  jaccard_score_value = jaccard_similarity(real_keyphrases_list, keybert_keyphrases_current)\n",
        "  jaccard_scores.append(jaccard_score_value)\n",
        "  print(\"Real Keyphrases:\", elem[\"keyphrases\"])\n",
        "  print(\"Predicted Keyphrases:\", keybert_keyphrases[-1])\n",
        "  print(\"Jaccard Similarity:\", jaccard_score_value)\n",
        "\n",
        "# Calcola la media della similarità di Jaccard\n",
        "average_jaccard_score = np.mean(jaccard_scores)\n",
        "print(\"Average Jaccard Similarity:\", average_jaccard_score)"
      ],
      "metadata": {
        "id": "HEGdwltjOX1A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}