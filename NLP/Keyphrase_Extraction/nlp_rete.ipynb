{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Nb9bsTkKKr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Dispositivo (MPS o CPU)\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Sto usando il dispositivo: {device}\")\n",
        "\n",
        "# Inizializzazione modelli\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "data = load_dataset(\"taln-ls2n/inspec\")[\"train\"].to_list()\n",
        "\n",
        "# Funzione di preprocessing\n",
        "def preprocess(text):\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "    return tokens\n",
        "\n",
        "def preprocess_keyphrase(text):\n",
        "    tokens = []\n",
        "    for t in text:\n",
        "        doc = t.replace(\"-\", \" \")\n",
        "        doc = nlp(doc.lower())\n",
        "        doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "        if len(doc) < 4 and len(doc) > 1:\n",
        "            tokens.append(\" \".join(doc))\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m05TSX1GkKKs"
      },
      "outputs": [],
      "source": [
        "# Liste finali dove accumuleremo features e labels\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "for doc in data:\n",
        "    title = doc['title']\n",
        "    abstract = doc['abstract']\n",
        "    keyphrases = doc['keyphrases']  # lista di stringhe\n",
        "    prmu = doc['prmu']\n",
        "    keyphrases = [kp for kp, p in zip(keyphrases, prmu) if p == \"P\"]\n",
        "    keyphrases = preprocess_keyphrase(keyphrases)\n",
        "\n",
        "    # Preprocessing del testo (title + abstract)\n",
        "    preprocessed_tokens = preprocess(title + '. ' + abstract)\n",
        "    processed_text = ' '.join(preprocessed_tokens)\n",
        "\n",
        "    # 1) Estraggo i candidati n-gram dal testo processato\n",
        "    vectorizer = CountVectorizer(ngram_range=(2, 3)).fit([processed_text])\n",
        "    candidates = vectorizer.get_feature_names_out()  # tutti gli n-gram\n",
        "\n",
        "    # 2) Embedding del documento\n",
        "    doc_embedding = embedder.encode(processed_text, show_progress_bar=False)\n",
        "    # Normalizziamo (per calcolo coseno semplificato)\n",
        "    doc_emb_norm = doc_embedding / norm(doc_embedding)\n",
        "\n",
        "    # 3) Embedding di tutti i candidati\n",
        "    candidate_embeddings = embedder.encode(candidates, show_progress_bar=False)\n",
        "    # Normalizzo ogni embedding di candidato (asse 1)\n",
        "    cand_emb_norm = candidate_embeddings / norm(candidate_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    # 4) Calcolo la similarità coseno tra doc e ciascun candidato\n",
        "    #    cos_sim = dot(u,v) / (||u|| * ||v||)\n",
        "    #    ma avendo già normalizzato doc_emb_norm e cand_emb_norm,\n",
        "    #    cos_sim = dot(doc_emb_norm, cand_emb_norm[i])\n",
        "\n",
        "    cos_sims = np.sum(doc_emb_norm * cand_emb_norm, axis=1)  # shape = (num_candidates,)\n",
        "\n",
        "    # 5) Etichette 1/0 in base alla corrispondenza letterale\n",
        "    #    (Se c e' ESATTAMENTE in doc['keyphrases'], label=1, altrimenti 0)\n",
        "    #    Attenzione: potresti dover normalizzare i Keyphrase reali per un match robusto.\n",
        "    labels = np.array([1 if c in keyphrases else 0 for c in candidates])\n",
        "\n",
        "    # 6) Creiamo le feature unendo doc_emb + candidate_emb (concatenazione)\n",
        "    combined_embeddings = np.hstack([\n",
        "        np.tile(doc_embedding, (len(candidates), 1)),\n",
        "        candidate_embeddings\n",
        "    ])\n",
        "\n",
        "    # 7) Negative sampling:\n",
        "    #    a) Prendiamo tutti i positivi\n",
        "    pos_indices = np.where(labels == 1)[0]\n",
        "    neg_indices = np.where(labels == 0)[0]\n",
        "\n",
        "    # Se il documento non ha nessun positivo, decidi come gestirlo:\n",
        "    # puoi saltarlo, o prendere qualche negativo a caso...\n",
        "    if len(pos_indices) == 0:\n",
        "        # Per esempio, skippo il documento\n",
        "        # continue\n",
        "        # Oppure prendo i top 10 negativi:\n",
        "        # neg_sorted = np.argsort(-cos_sims[neg_indices])\n",
        "        # top_neg = neg_sorted[:10]\n",
        "        # chosen_neg_indices = neg_indices[top_neg]\n",
        "        # final_feats = combined_embeddings[chosen_neg_indices]\n",
        "        # final_labels = labels[chosen_neg_indices]\n",
        "        # all_features.extend(final_feats)\n",
        "        # all_labels.extend(final_labels)\n",
        "        # continue  # e saltiamo i successivi passaggi\n",
        "        continue  # Saltiamo per semplificare\n",
        "\n",
        "    # b) Teniamo TUTTI i positivi\n",
        "    pos_feats = combined_embeddings[pos_indices]\n",
        "    pos_labels = labels[pos_indices]\n",
        "\n",
        "    # c) Ordiniamo i negativi per coseno discendente (i piu' simili in alto)\n",
        "    neg_sims = cos_sims[neg_indices]  # sim di tutti i negativi\n",
        "    neg_sorted_by_sim = np.argsort(-neg_sims)  # sort discendente\n",
        "\n",
        "    # d) Decidiamo quanti negativi tenere.\n",
        "    #    Esempio: 2 * (numero di positivi), i \"piu' simili\" (hard negatives)\n",
        "    n_pos = len(pos_indices)\n",
        "    keep_neg = n_pos * 4\n",
        "\n",
        "    chosen_neg = neg_sorted_by_sim[:keep_neg]  # prime 'keep_neg' posizioni\n",
        "    chosen_neg_indices = neg_indices[chosen_neg]\n",
        "\n",
        "    # e) Recuperiamo le feature e le label corrispondenti\n",
        "    neg_feats = combined_embeddings[chosen_neg_indices]\n",
        "    neg_labels = labels[chosen_neg_indices]\n",
        "\n",
        "    # 8) Combiniamo i positivi e i negativi selezionati\n",
        "    final_feats = np.concatenate([pos_feats, neg_feats], axis=0)\n",
        "    final_labels = np.concatenate([pos_labels, neg_labels], axis=0)\n",
        "\n",
        "    # 9) Aggiungiamo al dataset globale\n",
        "    all_features.extend(final_feats)\n",
        "    all_labels.extend(final_labels)\n",
        "\n",
        "# Ora all_features e all_labels contengono (doc + cand) + label,\n",
        "# con un numero ridotto di negativi per ridurre lo sbilanciamento.\n",
        "all_features = np.array(all_features)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "print(\"Dimensione finale delle feature:\", all_features.shape)\n",
        "print(\"Distribuzione delle etichette (0/1):\", np.bincount(all_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW2fxUSRkKKs"
      },
      "outputs": [],
      "source": [
        "# Indici positivi (label = 1)\n",
        "pos_indices = np.where(labels == 1)[0]\n",
        "\n",
        "# Se ci sono keyphrase positive...\n",
        "if len(pos_indices) > 0:\n",
        "    # Ordinamento discendente della similarità coseno (per i soli positivi)\n",
        "    # np.argsort(-array) li ordina in ordine decrescente\n",
        "    sorted_pos_indices = pos_indices[np.argsort(-cos_sims[pos_indices])]\n",
        "\n",
        "    # Scegliamo quanti ne vogliamo stampare (esempio: i top 5)\n",
        "    top_k = 5\n",
        "    best_pos_indices = sorted_pos_indices[:top_k]\n",
        "\n",
        "    print(\"Top candidati positivi (label=1) per similarità coseno:\")\n",
        "    for i in best_pos_indices:\n",
        "        print(f\"  - '{candidates[i]}' | coseno={cos_sims[i]:.4f}\")\n",
        "else:\n",
        "    print(\"Nessuna keyphrase positiva in questo documento.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjrNuchBkKKs"
      },
      "outputs": [],
      "source": [
        "# Creazione di una classe Dataset per gestire features e labels\n",
        "class KeyphraseDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features  # numpy array con forma (n_samples, 2 * base_emb_dim)\n",
        "        self.labels = labels      # numpy array con forma (n_samples,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return feature, label\n",
        "\n",
        "# Supponiamo di avere all_features e all_labels ottenuti dal preprocessing\n",
        "# all_features = np.array(all_features)   # forma (n_samples, 2 * base_emb_dim)\n",
        "# all_labels = np.array(all_labels)         # forma (n_samples,)\n",
        "# Per esempio:\n",
        "# all_features = np.random.rand(1000, 2 * base_emb_dim)\n",
        "# all_labels = np.random.randint(0, 2, size=(1000,))\n",
        "\n",
        "dataset = KeyphraseDataset(all_features, all_labels)\n",
        "batch_size = 512\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oyq-FGQkKKs"
      },
      "outputs": [],
      "source": [
        "class KeyphraseClassifierLSTMComplex(nn.Module):\n",
        "\n",
        "    def __init__(self, base_emb_dim, hidden_dim=512, num_layers=4, bidirectional=True, dropout=0.5):\n",
        "        super(KeyphraseClassifierLSTMComplex, self).__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # LSTM bidirezionale a più layer con dropout tra i layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=base_emb_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        # Calcoliamo la dimensione dell'output dell'LSTM\n",
        "        lstm_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "\n",
        "        # Meccanismo di attenzione: calcola pesi per ciascun output dell'LSTM\n",
        "        self.attention = nn.Linear(lstm_output_dim, 1)\n",
        "\n",
        "        # Layer fully connected con dropout per la classificazione finale\n",
        "        self.fc1 = nn.Linear(lstm_output_dim, 128)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x ha forma (batch_size, 2 * base_emb_dim)\n",
        "        # Lo rimodelliamo in una sequenza di lunghezza 2: (batch_size, 2, base_emb_dim)\n",
        "        batch_size = x.size(0)\n",
        "        x_seq = x.view(batch_size, 2, -1)\n",
        "\n",
        "        # Elaborazione con LSTM: lstm_out ha forma (batch_size, seq_len, lstm_output_dim)\n",
        "        lstm_out, _ = self.lstm(x_seq)\n",
        "\n",
        "        # Calcolo dei pesi di attenzione per ogni step della sequenza\n",
        "        attn_scores = self.attention(lstm_out)  # (batch_size, seq_len, 1)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)  # normalizziamo su seq_len\n",
        "\n",
        "        # Calcoliamo la rappresentazione pesata della sequenza\n",
        "        attn_output = torch.sum(attn_weights * lstm_out, dim=1)  # (batch_size, lstm_output_dim)\n",
        "\n",
        "        # Passaggio attraverso i layer fully connected con dropout\n",
        "        x_fc = self.fc1(attn_output)\n",
        "        x_fc = torch.relu(x_fc)\n",
        "        x_fc = self.dropout(x_fc)\n",
        "        x_fc = self.fc2(x_fc)\n",
        "        #out = self.sigmoid(x_fc)\n",
        "\n",
        "        return x_fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izZKWbjnkKKt"
      },
      "outputs": [],
      "source": [
        "# Conversione in tensori e trasferimento sul dispositivo\n",
        "X = torch.tensor(np.array(all_features), dtype=torch.float32).to(device)\n",
        "y = torch.tensor(np.array(all_labels), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "# Inizializzazione rete\n",
        "model = KeyphraseClassifierLSTMComplex(embedder.get_sentence_embedding_dimension()).to(device)\n",
        "pos_weight = torch.tensor([2.0]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "epochs = 60\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    for batch_features, batch_labels in dataloader:\n",
        "        batch_features = batch_features.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)  # outputs sono logit non sigmoidi\n",
        "        loss = criterion(outputs, batch_labels.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "    avg_loss = np.mean(epoch_losses)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            all_preds = []\n",
        "            all_true = []\n",
        "            for batch_features, batch_labels in dataloader:\n",
        "                batch_features = batch_features.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "                outputs = model(batch_features)\n",
        "                # Applico la sigmoid per ottenere probabilità\n",
        "                preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_true.append(batch_labels.cpu().numpy())\n",
        "            all_preds = np.concatenate(all_preds).flatten()\n",
        "            all_true = np.concatenate(all_true).flatten()\n",
        "\n",
        "            from sklearn.metrics import precision_score, recall_score\n",
        "            precision = precision_score(all_true, all_preds, zero_division=0)\n",
        "            recall = recall_score(all_true, all_preds, zero_division=0)\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iREMZXwnkKKt"
      },
      "outputs": [],
      "source": [
        "# Caricamento dati JSON Lines\n",
        "data = load_dataset(\"taln-ls2n/inspec\")[\"test\"].to_list()\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Funzione per calcolare la similarità di Jaccard\n",
        "def jaccard_similarity(list1, list2):\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    return intersection / union if union != 0 else 0.0\n",
        "\n",
        "def predict_keyphrases(model, data, embedder, threshold):\n",
        "    model.eval()  # pass in eval mode\n",
        "\n",
        "    # Se il modello è su GPU, assicuriamoci di spostare i dati su GPU\n",
        "    device = next(model.parameters()).device\n",
        "    jaccard_scores = []\n",
        "    for doc in data:\n",
        "        doc_id = doc.get('id', 'N/A')\n",
        "        title = doc.get('title', '')\n",
        "        abstract = doc.get('abstract', '')\n",
        "\n",
        "        # Preprocess\n",
        "        preprocessed_tokens = preprocess(title + \". \" + abstract)\n",
        "        processed_text = ' '.join(preprocessed_tokens)\n",
        "\n",
        "        # 1) Generiamo i candidati n-gram (1,2,3) dal testo preprocessato\n",
        "        vectorizer = CountVectorizer(ngram_range=(2,3)).fit([processed_text])\n",
        "        candidates = vectorizer.get_feature_names_out()\n",
        "\n",
        "        # 2) Embedding del documento\n",
        "        doc_embedding = embedder.encode(processed_text, show_progress_bar=False)\n",
        "\n",
        "        # 3) Embedding di tutti i candidati\n",
        "        cand_embeddings = embedder.encode(candidates, show_progress_bar=False)\n",
        "\n",
        "        # 4) Creiamo la concatenazione (doc + cand)\n",
        "        combined_features = []\n",
        "        for cand_emb in cand_embeddings:\n",
        "            feat = np.hstack([doc_embedding, cand_emb])\n",
        "            combined_features.append(feat)\n",
        "        combined_features = np.array(combined_features, dtype=np.float32)\n",
        "\n",
        "        # Converto in tensori PyTorch\n",
        "        X_test = torch.tensor(combined_features).to(device)\n",
        "\n",
        "        # 5) Ottengo le probabilità dal modello\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X_test)   # (num_candidates, 1)\n",
        "            probs = torch.sigmoid(outputs).float()\n",
        "\n",
        "        # 6) Stampo i candidati con prob >= threshold\n",
        "        pred = []\n",
        "        #predicted_keyphrases = []\n",
        "        for cand, prob in zip(candidates, probs):\n",
        "            if prob >= threshold:\n",
        "                pred.append(cand)\n",
        "                #predicted_keyphrases.append((cand, prob))\n",
        "\n",
        "        # Se vuoi stampare i risultati\n",
        "        #print(f\"\\nDocumento ID: {doc_id}\")\n",
        "        #print(f\"Title: {title}\")\n",
        "\n",
        "        # Indici posiivi (label = 1)\n",
        "        keyphrases = doc['keyphrases']  # lista di stringhe\n",
        "        prmu = doc[\"prmu\"]\n",
        "        keyphrases = [kp for kp, p in zip(keyphrases, prmu) if p == \"P\"]\n",
        "        keyphrases = preprocess_keyphrase(keyphrases)\n",
        "\n",
        "        # Filtraggio: per ogni keyphrase, se la parola iniziale è già presente, mantieni quella con prob maggiore\n",
        "        \"\"\"filtered_keyphrases = {}\n",
        "        for cand, prob in predicted_keyphrases:\n",
        "            first_word = cand.split()[0]  # prendi la prima parola della keyphrase\n",
        "            # Se la parola non è ancora presente oppure la nuova ha probabilità maggiore, aggiorna il dizionario\n",
        "            if (first_word not in filtered_keyphrases) or (prob.item() > filtered_keyphrases[first_word][1].item()):\n",
        "                filtered_keyphrases[first_word] = (cand, prob)\"\"\"\n",
        "\n",
        "        # Converti il dizionario in una lista di tuple per la stampa\n",
        "        #filtered_list = list(filtered_keyphrases.values())\n",
        "\n",
        "        jaccard_score_value = jaccard_similarity(pred, keyphrases)\n",
        "        jaccard_scores.append(jaccard_score_value)\n",
        "\n",
        "        #print(f\"Jaccard score: {jaccard_score_value}\")\n",
        "\n",
        "        #print(\"Keyphrase predette filtrate:\")\n",
        "        #for cand, prob in filtered_list:\n",
        "            #print(f\" - '{cand}' (prob={prob.item():.2f})\")\n",
        "\n",
        "        #print(f\"Keyphrase reali\")\n",
        "        #for k in keyphrases:\n",
        "            #print(f\"- {k}\")\n",
        "    average_jaccard_score = np.mean(jaccard_scores)\n",
        "    print(\"Average Jaccard Similarity:\", average_jaccard_score)\n",
        "\n",
        "# Supponiamo di aver definito 'test_data' come lista di doc con id, title, abstract, ...\n",
        "predict_keyphrases(model, data, embedder, threshold=0.5)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}